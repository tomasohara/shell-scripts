#!/usr/bin/env python3
#
# SCRIPT_NAME: batspp_report.py
# DESCRIPTION: Test automation & report generation for ipynb test files (for BatsPP 1.5.X).
# BatsPP files are generated by default (stored at ./batspp-only).
#
# TODO2:
# - use gh.form_path instead of f"{dir}/{file}" (for sake of Windows users).
#
# Note:
# - By default, this aborts if run under an admin-like account (e.g., root or power user), because the tests might inadvertantly delete files.
#

"""
  Test automation & report generation for BatsPP test files for Bash.
  The files can be generated from Jupyter .ipynb files (see ./batspp-only).
"""

# Standard modules
import math

# Installed modules
import yaml

# Local modules
from mezcla import debug
from mezcla import glue_helpers as gh
from mezcla import system
msy = system
from mezcla.main import Main
from mezcla.my_regex import my_re
try:
    from simple_batspp import FORCE_RUN
except:
    system.print_exception_info("simple_batspp.FORCE_RUN")
    parent_dir = system.real_path(".")
    system.exit(f"Error: make sure PYTHONPATH includes parent dir:\n\t{parent_dir}")

# Constants
TL = debug.TL

## TEMP
## pylint: disable=f-string-without-interpolation

IPYNB = ".ipynb"
BATSPP = ".batspp"
BATS = ".bats"
## OLD: TXT = ".txt"
TXT = ".summary.txt"
NOBATSPP = "NOBATSPP"
## OLD: OUTPUTPP = ".outputpp"
OUTPUTPP = ".bats.outputpp"

OUTPUT_DIR = system.getenv_text(
    "OUTPUT_DIR", ".",
    description="Directory for output files such as .batspp test specs")

BATSPP_STORE = gh.form_path(OUTPUT_DIR, "batspp-only")
BATS_STORE = gh.form_path(OUTPUT_DIR, "bats-only")
BATS_STORE = BATSPP_STORE
TXT_STORE = gh.form_path(OUTPUT_DIR, "txt-reports")
KCOV_STORE = gh.form_path(OUTPUT_DIR, "kcov-output")
BATSPP_OUTPUT_STORE = gh.form_path(OUTPUT_DIR, "batspp-output")

ESSENTIAL_DIRS = [BATSPP_STORE, BATS_STORE, KCOV_STORE, TXT_STORE, BATSPP_OUTPUT_STORE]
## TODO:
## ESSENTIAL_DIRS = map(gh.form_path(TMP, filename)
##                      for filename in [BATSPP_STORE, BATS_STORE, KCOV_STORE, TXT_STORE, BATSPP_OUTPUT_STORE])
ESSENTIAL_DIRS_present = all(gh.file_exists(dir) for dir in ESSENTIAL_DIRS)
THRESHOLDS_FILE = "thresholds.yaml"
DEFAULT_MIN_SCORE = 50

# Environment options
# Note: These are just intended for internal options, not for end users.
# It also allows for enabling options in one place rather than four
# (e.g., [Main member] initialization, run-time value, and argument spec., along
# with string constant definition).
#

HOME = system.getenv_text(
    "HOME", "~",
    description="Home directory for user")
DOCKER_HOME = "/home/shell-scripts"
UNDER_DOCKER = ((HOME == DOCKER_HOME) or system.file_exists(DOCKER_HOME))
UNDER_RUNNER = ("/home/runner" in HOME)
UNDER_TESTING_VM = system.getenv_text(
    "UNDER_TESTING_VM", (UNDER_DOCKER or UNDER_RUNNER),
    description="Whether to assume running under testing VM")
DETAILED_DEBUGGING = debug.detailed_debugging()

TEST_REGEX = system.getenv_value(
    ## TODO2: rename to BATSPP_TEST_REGEX
    "TEST_REGEX", None,
    description="Regex for tests to include; ex: '^c.*' for debugging")
TRACE_EXCERPT_SIZE = system.getenv_int(
    "TRACE_EXCERPT_SIZE", 1000,
    description="Number of lines traced with trace_excerpt")
SHOW_FAILURE_CONTEXT = system.getenv_bool(
    "SHOW_FAILURE_CONTEXT", UNDER_TESTING_VM,
    description="Show context of test failures--useful with Github runner")
SINGLE_STORE = system.getenv_bool(
    "SINGLE_STORE", False,
    description=f"Whether to just use {BATSPP_OUTPUT_STORE} for all store dirs except kcov")
CLEAN_DEFAULT = system.getenv_bool(
    "CLEAN_OUTPUT", not DETAILED_DEBUGGING,
    description=f"Whether to clean existing output by remove entire directories")
TEST_DIR = system.getenv_value(
    "TEST_DIR", None,
    description="Directory with BatsPP test definitions")
STRICT_EVAL = system.getenv_bool(
    "STRICT_EVAL", False,
    description="Use strict evaluation model, such as without whitespace normalization")
BATS_EVAL = system.getenv_bool(
    "BATS_EVAL", False,
    description="Evaluate tests via bats rather than bash")
BASH_EVAL = (not BATS_EVAL)
OMIT_SHORT_OPTIONS = system.getenv_bool(
    "OMIT_SHORT_OPTIONS", False,
    description="Only allow long command line options")
ALLOW_SHORT_OPTIONS = (not OMIT_SHORT_OPTIONS)
## BAD:
## FORCE_RUN = system.getenv_bool("FORCE_RUN", False,
##     "Force execution even if admin-like user")
FORCE_OPTION_DEFAULT = UNDER_DOCKER or FORCE_RUN

## NOTE: the code needs to be thoroughly revamped (e.g., currently puts .batspp in same place as .bats)
if SINGLE_STORE:
    BATSPP_STORE = BATS_STORE = TXT_STORE = BATSPP_OUTPUT_STORE
    debug.trace(3, f"FYI: Using single store for text-based output files, etc.: {BATSPP_STORE}")

#-------------------------------------------------------------------------------

def load_thresholds(filename):
    """Load test failure thresholds from a YAML file."""
    # TODO: put in utility module
    result = {}
    with open(filename, "r", encoding="utf-8") as yamlfile:
        result = yaml.safe_load(yamlfile)
    debug.trace(5, f"load_thresholds({filename}) => {result}")
    return result

def round3(num):
    """Round NUM to 3 decimal places with result returned as string"""
    # EX: round3(0.12) => "0.123"
    return system.round_as_str(num, precision=3)

#-------------------------------------------------------------------------------
# NOTE: *** This function is too monolithinc: it should be structured like kcov_result.py ***
# TODO2: Convert batspp-proper logic into a class with a separate method for each step; the
#    __init__ method can take one argument per command-line option to facilitate conversion).
#

def main():
    """Entry point"""

    # Get files in test definition dir
    # note: uses TEST_DIR or basename of this file, with "." as fallback
    ## OLD: files = msy.read_directory(".")
    test_path = system.real_path(TEST_DIR or ".")
    # note: script should either run in ./tests dir or define TEST_DIR
    ## OLD: debug.assertion(test_path.endswith("tests"))
    files = msy.read_directory(test_path)

    # 0.1) CHECKING IF THE DIRECTORY EXISTS
    if not ESSENTIAL_DIRS_present:
        for dir_path in ESSENTIAL_DIRS:
            gh.full_mkdir(dir_path)

    batspp_count = 0

    # 0.2) Option Parsing
    # note: --omit-reports used instead of --no-reports for sake of auto-short options
    NO_REPORTS_ARG = "omit-reports"
    NO_REPORTS_ALIAS_ARG = "no"
    KCOV_REPORTS_ARG = "kcov"
    TEXT_REPORTS_ARG = "txt"
    ALL_REPORTS_ARG = "all"
    BATSPP_SWITCH_ARG = "switch"
    FORCE_ARG = "force"
    CLEAN_ARG = "clean"
    DEFINITIONS_ARG = "definitions"
    main_app = Main(
        description=__doc__.format(script=gh.basename(__file__)).strip("\n"),
        boolean_options=[
            (NO_REPORTS_ARG, "No reports are generated, testfiles are shown"),
            (NO_REPORTS_ALIAS_ARG, f"Alias for --{NO_REPORTS_ARG}"),
            (KCOV_REPORTS_ARG, f"KCOV (HTML based) reports generated, stored at {KCOV_STORE}"),
            (TEXT_REPORTS_ARG, f"Textfile based reports generated, stored at {TXT_STORE}"),
            (ALL_REPORTS_ARG, "Generates report for all available testfiles (n.b., NOBATSPP testfiles ignored by default)"),
            (FORCE_ARG, "Force running under admin-like account"),
            (CLEAN_ARG, "Remove output from previous runs; *** warning: this removes entire subdirectories"),
            (BATSPP_SWITCH_ARG, "Uses batspp library instead of ../simple_batspp.py script"),
        ],
        text_options=[
            (DEFINITIONS_ARG, "Script with alias definitions to be sourced"),
        ],
        skip_input=True,
        manual_input=False,
        auto_help=True,
        short_options=ALLOW_SHORT_OPTIONS,
    )
    #
    debug.assertion(main_app.parsed_args)
    NO_OPTION_ALIAS = main_app.get_parsed_option(NO_REPORTS_ALIAS_ARG)
    NO_OPTION = main_app.get_parsed_option(NO_REPORTS_ARG, NO_OPTION_ALIAS)
    TXT_OPTION = main_app.get_parsed_option(TEXT_REPORTS_ARG)
    KCOV_OPTION = main_app.get_parsed_option(KCOV_REPORTS_ARG)
    ALL_OPTION = main_app.get_parsed_option(ALL_REPORTS_ARG)
    FORCE_OPTION = main_app.get_parsed_option(FORCE_ARG, UNDER_TESTING_VM or FORCE_RUN)
    CLEAN_OPTION = main_app.get_parsed_option(CLEAN_ARG, CLEAN_DEFAULT)
    BATSPP_SWITCH_OPTION = main_app.get_parsed_option(BATSPP_SWITCH_ARG)
    USE_SIMPLE_BATSPP = (not BATSPP_SWITCH_OPTION)
    DEFINITIONS_SCRIPT = main_app.get_parsed_option(DEFINITIONS_ARG)
    debug.trace_expr(4, NO_OPTION, TXT_OPTION, KCOV_OPTION, FORCE_OPTION, CLEAN_OPTION, BATSPP_SWITCH_OPTION, USE_SIMPLE_BATSPP, DEFINITIONS_SCRIPT)
    RUN_BATS = (TXT_OPTION or not NO_OPTION)
    debug.assertion(TXT_OPTION or KCOV_OPTION)

    # Do check for adminstrative user and exit unless --force
    is_admin = my_re.search(r"root|admin|adm", gh.run("groups"))
    if is_admin:
        if not FORCE_OPTION:
            msy.exit("Error: running under admin account requires --force option, unless under testing VM (e.g., docker shell-scripts-dev image)")
        msy.print_stderr("FYI: not recommended to run under admin account")

    # Cleanup up previous runs
    # Warning: 'rm -rf' is a very dangerous command:
    # it should only be done in temporary directories (e.g., not under repo)
    if CLEAN_OPTION:
        if RUN_BATS:
            gh.run(f"rm -rf {BATSPP_STORE}/*")
            gh.run(f"rm -rf {BATS_STORE}/*")
        
        if KCOV_OPTION:
            gh.run(f"rm -rf {KCOV_STORE}/*")
    
        if TXT_OPTION:
            gh.run(f"rm -rf {TXT_STORE}/*")

    def trace_excerpt(filename, level=None, num_lines=None):
        """Outputs the first NUM_LINES in FILENAME if at debugging trace LEVEL
        Note: that output goes to stdout not stderr (unlike debug.trace)"""
        if level is None:
            level = TL.VERBOSE
        if num_lines is None:
            num_lines = TRACE_EXCERPT_SIZE
        if debug.debugging(level):
            print(gh.run(f"echo BatsPP source:; head -{num_lines} --verbose {filename} | cat -n"))
        return
            
    def run_batspp(input_file, output_file):
        """Run BatsPP over INPUT_FILE to produce OUTPUT_FILE"""
        # Note: for convenience, output is written to disk and returned by the function
        ## TODO2: add log_file argument for stderr; change output_file to output_script_file
        debug.assertion(not my_re.search(r"\s", output_file))
        real_output_file = output_file + ".out"
        log_file = output_file + ".log"

        # NEW / TODO: Added resolved path for check_batspp.perl
        check_errors_script = gh.resolve_path("../check_errors.perl")
        
        debug.trace(5, f"run_batspp{(input_file, output_file)}")
        source_spec = (f"--source '{DEFINITIONS_SCRIPT}'" if DEFINITIONS_SCRIPT else "")
        if USE_SIMPLE_BATSPP:
            # note: adds sentinels around paragraph segments for simpler parsing;
            # uses Bash instead of Bats (to bypass need for global setup sections);
            # copies ./tests files into bats test dir (under temp); retains outer
            # quotation marks in output; uses single test directory; passes along --force option
            eval_log = output_file + ".eval.log"
            lenient_eval = (not STRICT_EVAL)
            run_output = gh.run(f"MATCH_SENTINELS=1 PARA_BLOCKS=1 BASH_EVAL={BASH_EVAL} COPY_DIR=1 KEEP_OUTER_QUOTES=1 GLOBAL_TEST_DIR=1 FORCE_RUN={FORCE_OPTION} EVAL_LOG={eval_log} NORMALIZE_WHITESPACE={lenient_eval} STRIP_COMMENTS={lenient_eval} IGNORE_SETUP_OUTPUT=1 python3 ../simple_batspp.py {input_file} --output {output_file} {source_spec} > {real_output_file} 2> {log_file}")
        else:
            run_output = gh.run(f"batspp {input_file} --save {output_file} {source_spec} > {real_output_file} 2> {log_file}")
        # Output excerpts from BatsPP source file, Bats output file and conversion log file.
        # The corresponding extensions follow: .batspp, .bats.outputpp, and .bats.outputpp.log
        # note: BatsPP now output above
        ## OLD: trace_excerpt(input_file, level=6)
        trace_excerpt(output_file)
        trace_excerpt(log_file, level=4)
        # Check for common errors (e.g., command not found or insufficient permissions)
        ## EXP: print(gh.run(f"{check_errors_script} {log_file}"))
        
        ## TEMP: Show context of failed tests for help with diagnosis of Github actions runs (as temp files not accessible afterwards)
        if SHOW_FAILURE_CONTEXT:
            context = gh.run(f"grep -B10 '^not ok' {real_output_file}")
            print("Failure context: " + (f"{{\n{gh.indent_lines(context)}}}" if context.strip() else "n/a"))
        # Output excerpt of BatsPP output and log file (i.e., stdout and stderr)
        # The corresponding extensions follow: .bats.outputpp.out, and .bats.outputpp.eval.log.
        trace_excerpt(real_output_file)
        trace_excerpt(eval_log)
        debug.assertion(not run_output.strip())
    
        real_output = system.read_file(real_output_file)
        eval_errors = (gh.run(f"{check_errors_path} -context=0 {log_file}").split("\n"))[1:]
        
        print(f"\nEvaluation Errors: {len(eval_errors)}")
        if (len(eval_errors) > 0):
            print("\nError Details:\n")
            for error in eval_errors:
                print("\t"+error+"\n")
            print("")
        
        debug.trace(6, f"\nrun_batspp() ##real_output## => {real_output!r}")
        debug.trace(6, f"\nrun_batspp() ##eval_errors## => {eval_errors!r}")
        debug.trace(6, f"\nrun_batspp() ##len(eval_errors)## => {len(eval_errors)!r}\n")                                                  
        return real_output, eval_errors

    ## TEST (plan A until COPY_DIR=1 added above)
    ## # 0.9) Making sure input files, etc. accessible in bats directory
    ## # TODO2: use directory for (e.g., ./resources)
    ## debug.trace(4, "Copying over potential input files into bats directory")
    ## gh.run(f"cp -vf *.html *.input* *.yaml *.list *.rst *.txt *.text {BATS_STORE}/")

    # Other initialization

    if system.file_exists(THRESHOLDS_FILE):
        thresholds = load_thresholds(THRESHOLDS_FILE)
        missing_test_files = [f for f in thresholds.keys() if not system.file_exists(f)]
        debug.assertion(not missing_test_files,
                        f"Extraneous file(s) in {THRESHOLDS_FILE}: {missing_test_files}")
    
    # 1) Identifying .ipynb files
    i = 1
    ipynb_array = []
    avoid_array = []
    avoid_count = 0
    success_test_array = []
    failure_test_array = []

    batspp_version = gh.extract_match_from_text(r"version (\S+)",
                                                gh.run("batspp --version 2> /dev/null"))
    print("\n" + "-"*35)
    print(f"\nBATSPP Report Generation (simple_batspp.py 1.5.x or BATSPP {batspp_version})\n")
    print("-"*35)
    ## TEMP (tracing test for act/nektos)
    print(f"TEST_REGEX={TEST_REGEX} DEBUG_LEVEL={system.getenv('DEBUG_LEVEL')}")
    debug.trace_expr(1, OUTPUT_DIR, SINGLE_STORE)

    for file in files:
        is_ipynb = file.endswith(IPYNB)
        if is_ipynb:
            if TEST_REGEX and not my_re.search(fr"{TEST_REGEX}", file):
                debug.trace(3, f"FYI: Ignoring {file} not matching TEST_REGEX ({TEST_REGEX})")
                continue
            if not ALL_OPTION and NOBATSPP in file:
                debug.trace(4, f"FYI: Ignoring NOBATSPP file: {file}")
                print(f"NOBATSPP File Found [{i}]: {file}")
                avoid_array.append(file)
                avoid_count += 1
                continue
            print(f"JUPYTER Testfile Found [{i}]: {file}")
            ipynb_array.append(file)
            i += 1

    print(f"\nIPYNB Files Found (Total - NOBATSPP): {i-1} - {avoid_count} = {i-avoid_count-1}")

    # 2) Generating .batspp files from .ipynb files
    i = 1
    batspp_array = []
    print(f"\nGeneration of BATSPP files\n")

    for testfile in ipynb_array:
        is_ipynb = testfile.endswith(IPYNB)
        if is_ipynb:
            ## OLD: batspp_from_ipynb = testfile.replace(IPYNB, BATSPP)
            batspp_from_ipynb = gh.form_path(BATSPP_STORE,
                                             gh.basename(testfile.replace(IPYNB, BATSPP)))
            print(f"IPYNB TESTFILE [{i}]: {testfile} => {batspp_from_ipynb}")
            log_file = f"{batspp_from_ipynb}.log"
            ## OLD: gh.run(f"python3 ../jupyter_to_batspp.py {testfile} --output {batspp_from_ipynb} 2> {log_file}")
            extra_args = ("--add-annots" if DETAILED_DEBUGGING else "")
            gh.run(f"python3 ../jupyter_to_batspp.py {extra_args} --output {batspp_from_ipynb} {testfile} 2> {log_file}")
            debug.call(4, gh.run, f"check_errors.perl {log_file}", **{"output": True})
            # note: trace jupyter-to-batspp conversion and log; only if "quite detailed" (6) tracing
            # The corresponding extensions follow: .batspp, .batspp.log
            trace_excerpt(batspp_from_ipynb, level=6)
            trace_excerpt(log_file, level=6)
            batspp_array.append(batspp_from_ipynb)
            i += 1
    ipynb_count = i - 1
    debug.trace_expr(5, ipynb_count, batspp_array)
    debug.assertion(ipynb_count == len(batspp_array))

    # 3) Executing batspp files & storing them as bats
    ## OLD: print(f"\n\n==========BATS GENERATED==========\n")
    print("\n" + "-"*25)
    print("BATS FILE GENERATION:")
    print("-"*25 + "\n")

    # TODO3: rename i => num_test_files, total_count_ok => total_ok_tests, and total_count_total to total_num_tests
    i = 1
    total_count_ok = 0
    total_count_total = 0
    total_success_rate = 0
    total_num_successful = 0

    if not RUN_BATS:
        ## OLD: print(f"- SKIPPING BATSPP CHECK (-n ARGUMENT PROVIDED)\n")
        print("Skipping BATSPP Check (-n option)\n")
    else:
        for batsppfile_path in batspp_array:
            batsppfile = gh.basename(batsppfile_path)
            ipynb_from_batspp = batsppfile.replace(BATSPP, IPYNB)
            bats_from_batspp = batsppfile.replace(BATSPP, BATS)
            output_from_batspp = batsppfile.replace(BATSPP, OUTPUTPP)
            txt_from_batspp = batsppfile.replace(BATSPP, TXT)
            test_extensionless = gh.remove_extension(batsppfile, BATSPP)
            is_batspp = batsppfile.endswith(BATSPP)
            debug.trace_expr(4, batsppfile, ipynb_from_batspp)
            
            if not is_batspp:
                debug.trace(4, f"Ignoring non-batspp file {batsppfile}")
                continue
            print("\n"+"="*35)
            ## OLD: print(f"BATSPP FILE DETECTED [{i}]: {batsppfile} => {bats_from_batspp}\n")
            print(f"BATSPP File {i}: {bats_from_batspp}")
            i += 1

            if TXT_OPTION:
                output_from_batspp_path = gh.form_path(BATSPP_OUTPUT_STORE, output_from_batspp)
                ## OLD: run_bat
                bats_output, eval_errors = run_batspp(batsppfile_path, output_from_batspp_path)
                num_eval_errors = len(eval_errors)

                output_lines = bats_output.splitlines()
                output_lines_filtered = [item for item in output_lines if not item.startswith("#")]
                debug.trace_expr(5, output_lines_filtered)
                if output_lines_filtered:
                    # Ignore the line given the number of tests (e.g., "1..5")
                    header_line = output_lines_filtered.pop(0)
                    debug.trace_expr(5, header_line)
                    debug.assertion(my_re.search(r"^1\.\.\d+", header_line) or (header_line == "0..0"),
                                    f"Unexpected header line for {output_from_batspp_path}: {header_line!r}")
                debug.assertion(len(output_lines_filtered))
                
                count_ok = len([item for item in output_lines_filtered if item.startswith("ok")])
                count_bad = len([item for item in output_lines_filtered if item.startswith("not ok")])
                count_total = (count_ok + count_bad)
                success_rate = (round((count_ok / count_total)*100, 2) if count_total else 0)
                min_score = system.to_float(thresholds.get(ipynb_from_batspp, DEFAULT_MIN_SCORE))
                successful = (success_rate >= min_score)
                debug.trace_expr(4, min_score, count_ok, count_bad, count_total, success_rate, successful)
                ## OLD: SUMMARY_TEXT = f"{count_ok} out of {count_total} successful ({success_rate}%)\nSuccess: {successful} (threshold = {min_score}%)"
                SUMMARY_TEXT_SUCCESS_COUNT = f"Success Score: {count_ok} out of {count_total} ({success_rate}%)"
                SUMMARY_TEXT_SUCCESS_STATUS = f"Success Status: {successful} (Threshold: {min_score}%)"
                msy.write_file(f"{TXT_STORE}/{txt_from_batspp}", SUMMARY_TEXT_SUCCESS_COUNT+"\n"+SUMMARY_TEXT_SUCCESS_STATUS)
                ## OLD: print(f"{test_extensionless}: {SUMMARY_TEXT}" + "\n" + "="*35 + "\n")
                print(f"Test Results:\n- {SUMMARY_TEXT_SUCCESS_COUNT}\n- {SUMMARY_TEXT_SUCCESS_STATUS}")
                total_count_ok += count_ok
                total_count_total += count_total
                total_success_rate += success_rate
                total_num_successful += int(successful)

                # Categorizing Tests if they are successful or not
                txt_option_JSON = {}
                txt_option_JSON["test_name"] = test_extensionless
                txt_option_JSON["test_count_total"] = count_total
                txt_option_JSON["test_count_ok"] = count_ok
                txt_option_JSON["test_min_score"] = min_score
                txt_option_JSON["test_success_rate"] = success_rate
                txt_option_JSON["test_count_eval_error"] = num_eval_errors
                ## NEW: Inorder for test to be successful, it must satisfy the condition below:
                ## min_score means threshold of test
                ## NEW: Updated if(successful) -> if(successful and (count_total != 0 or min_score == 0))
                if successful and (count_total != 0 or min_score == 0):  
                    success_test_array.append(txt_option_JSON)
                else:
                    failure_test_array.append(txt_option_JSON)

            if KCOV_OPTION:
                # TODO2: extend run_batspp to handle optional coverage check
                ## OLD: bats_program = ("python3 ../simple_batspp.py" if USE_SIMPLE_BATSPP else "bats")
                # note: kcov works best over the Bash version of the test file not batspp (i.e., BASH_EVAL=1)
                bats_program = "bats"
                if USE_SIMPLE_BATSPP and BASH_EVAL:
                    bats_program = ""
                run_batspp(batsppfile_path, f"{BATS_STORE}/{bats_from_batspp}")
                gh.run(f"kcov {KCOV_STORE}/{test_extensionless} {bats_program} {BATS_STORE}/{bats_from_batspp}")
                    
                KCOV_MESSAGE = f"KCOV REPORT PATH: {KCOV_STORE}/{test_extensionless}/"
                print(gh.indent(KCOV_MESSAGE, indentation="  -  ", max_width=512))
                gh.run(f"kcov {KCOV_STORE}/{test_extensionless} {bats_program} {BATS_STORE}/{bats_from_batspp}")

        batspp_count = i - 1 

    # 4) Mentioning any errors in ipynb testfiles
    working_testfiles = []
    sucess_test_array_SORT = sorted(success_test_array, key=lambda x: x["test_success_rate"], reverse = True)
    failure_test_array_SORT = sorted(failure_test_array, key=lambda x: x["test_success_rate"], reverse = True)

    for file in batspp_array:
        is_batspp = file.endswith(BATSPP)

        if is_batspp:
            ## Lorenzo Review: is there a reason im missing why this line doesn't do .replace(BATSPP, IPYNB) as the other replaces?
            BI_check = [file.replace(".batspp", ".ipynb")]
            working_testfiles += BI_check

    # 5) Summary Statistics
    set_wt = set(working_testfiles)
    error_testfiles = [tf for tf in ipynb_array if tf not in set_wt]
    faulty_count = ipynb_count - batspp_count

    NaN = math.nan
    print(f"\n======================================================")
    print(f"SUMMARY STATISTICS:\n")
    print(f"simple_batspp.py used: {bool(USE_SIMPLE_BATSPP)}")
    print(f"No. of IPYNB testfiles: {ipynb_count + avoid_count}")
    print(f"No. of BATSPP files (generated): {batspp_count if RUN_BATS else NaN}")
    print(f"No. of FAULTY testfiles: {faulty_count if RUN_BATS else NaN}")
    print(f"No. of AVOIDED testfiles: {avoid_count}")
    print(f"Total no. of good tests: {total_count_ok}")
    print(f"Total no. of individual tests: {total_count_total}")
    # Note: macro-average is mean of success score, whereas micro-average is based on global counts.
    # See https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin
    avg_successful = macro_success_rate = micro_success_rate = NaN
    if batspp_count:
        avg_successful = total_num_successful / batspp_count * 100
        macro_success_rate = total_success_rate / batspp_count
    if total_count_total:
        micro_success_rate = total_count_ok / total_count_total * 100
    print(f"Total no. files OK w/ threshold: {total_num_successful}")
    print(f"Average no. files OK w/ threshold: {round3(avg_successful)}%")
    print(f"Macro success score: {round3(macro_success_rate)}% ({round3(total_success_rate)}/{batspp_count * 100})")
    print(f"Micro success score: {round3(micro_success_rate)}% ({total_count_ok}/{total_count_total})")
    print("    where successful based on threshold, macro is mean of individual scores, and micro is global metric")

    print(f"\nFAULTY TESTFILES:")
    if faulty_count == 0:
        print("n/a")
    else:
        for tf in error_testfiles:
            print(f"- {tf}")
            
    print("\nAVOIDED TESTFILES:")
    if avoid_count == 0:
        print("n/a")
    else:
        for tf in avoid_array:
            print(f"- {tf}")

    if TXT_OPTION:
        print("\nTEST SUCCESS (--txt ENABLED):")
        print(f"No. of Successful Tests:", len(success_test_array))
        print(f"No. of Failure Tests:", len(failure_test_array))
        debug.assertion(len(success_test_array) == total_num_successful)

        def print_test_array(arr):
            """Print summary of test results in ARR"""
            for index, item in enumerate(arr):
                t_name = item['test_name']
                t_rate = item['test_success_rate']
                t_min_score = item['test_min_score']
                t_count_total = item['test_count_total']
                t_count_ok = item['test_count_ok']
                t_count_eval_error = item['test_count_eval_error']
                ## OLD: Revised format includes test passed out of test found
                # print(f"{index + 1}. {name} ({rate}%): threshold={min_score}%")
                print(f"{index+1}. {t_name} ({t_rate}%; {t_count_ok}/{t_count_total} OK): threshold={t_min_score}%; {t_count_eval_error} evaluation errors")
            if not arr:
                print("n/a")

        print("\nSuccessful Tests:\n---------------------")
        print_test_array(sucess_test_array_SORT)
        print("\nFailure Tests:\n---------------------")
        print_test_array(failure_test_array_SORT)
        
    print(f"======================================================")

    # Return number of failed tests as statues (i.e., OK if 0 failed)
    code = (len(failure_test_array) if success_test_array else -1)
    system.exit(status_code=code)
    
# -------------------------------------------------------------------------------

if __name__ == '__main__':
    debug.trace_current_context(level=TL.QUITE_DETAILED)
    main()
